{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi CPUS/GPUs support with Horovod\n",
    "\n",
    "> **Warning** \n",
    "> The use  mutli-GPU is under developpement and hasn't been thoroughly tested yet. Proceed with caution !\n",
    "\n",
    "QMC simulations can easily be parallelized by using multiple ressources to sample the wave function. Each walker is indenpendent of the other ones and therefore multiple compute node can be used in parallel to obtain more samples. Each node can alsu use GPUs is they are available. We demonstrate here how to use the library `Horovod` (https://github.com/horovod/horovod) to leverage large compute ressources for QMC.\n",
    "\n",
    "Let's first create a simple system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from qmctorch.scf import Molecule\n",
    "from qmctorch.wavefunction import SlaterJastrow\n",
    "from qmctorch.sampler import Metropolis\n",
    "from qmctorch.utils import (plot_energy, plot_data)\n",
    "from qmctorch.utils import set_torch_double_precision\n",
    "set_torch_double_precision()\n",
    "mol = Molecule(atom='H 0. 0. 0; H 0. 0. 1.', unit='bohr', redo_scf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if GPUs are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = SlaterJastrow(mol, cuda=use_gpu).gto2sto()\n",
    "sampler = Metropolis(nwalkers=100, nstep=500, step_size=0.25,\n",
    "                     nelec=wf.nelec, ndim=wf.ndim,\n",
    "                     init=mol.domain('atomic'),\n",
    "                     move={'type': 'all-elec', 'proba': 'normal'},\n",
    "                     cuda=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dict = [{'params': wf.jastrow.parameters(), 'lr': 3E-3},\n",
    "           {'params': wf.ao.parameters(), 'lr': 1E-6},\n",
    "           {'params': wf.mo.parameters(), 'lr': 1E-3},\n",
    "           {'params': wf.fc.parameters(), 'lr': 2E-3}]\n",
    "opt = optim.Adam(lr_dict, lr=1E-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dedicated QMCTorch Solver has been developped to handle multiple GPU. To use this solver simply import it\n",
    "and use is as the normal solver and only a few modifications are required to use horovod :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import horovod.torch as hvd\n",
    "from qmctorch.solver import SolverMPI\n",
    "\n",
    "hvd.init()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(hvd.rank())\n",
    "    \n",
    "solver = SolverMPI(wf=wf, sampler=sampler,\n",
    "                                    optimizer=opt,\n",
    "                                    rank=hvd.rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.configure(track=['local_energy'], freeze=['ao', 'mo'],\n",
    "                loss='energy', grad='auto',\n",
    "                ortho_mo=False, clip_loss=False,\n",
    "                resampling={'mode': 'update',\n",
    "                            'resample_every': 1,\n",
    "                            'nstep_update': 50})\n",
    "\n",
    "# optimize the wave function\n",
    "obs = solver.run(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see some classes need the rank of the process when they are defined. This is simply\n",
    "to insure that only the master process generates the HDF5 files containing the information relative to the calculation.\n",
    "\n",
    "## Running parallel calculations\n",
    "\n",
    "It is currently difficult to use Horovod on mutliple node through a jupyter notebook. To do so, one should have a python file with all the code and execute the code  with the following command\n",
    "\n",
    "```\n",
    "horovodrun -np 2 python <example>.py\n",
    "```\n",
    "\n",
    "See the horovod documentation for more details : https://github.com/horovod/horovod\n",
    "\n",
    "\n",
    "This solver distribute the `Nw` walkers over the `Np` process . For example specifying 2000 walkers\n",
    "and using 4 process will lead to each process using only 500 walkers. During the optimizaiton of the wavefunction\n",
    "each process will compute the gradients of the variational parameter using their local 500 walkers.\n",
    "The gradients are then averaged over all the processes before the optimization step takes place. This data parallel\n",
    "model has been greatly succesfull in machine learning applications (http://jmlr.org/papers/volume20/18-789/18-789.pdf)\n",
    "\n",
    "A complete example can found in `qmctorch/docs/example/horovod/h2.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
